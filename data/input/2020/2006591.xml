<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: AF: Small: Parallel Reinforcement Learning with Communication and Adaptivity Constraints]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>242214.00</AwardTotalIntnAmount>
<AwardAmount>242214</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Brass</SignBlockName>
<PO_EMAI>pbrass@nsf.gov</PO_EMAI>
<PO_PHON>7032922182</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[Reinforcement learning has witnessed great research advancement in recent years and achieved successes in many practical applications.  However, reinforcement-learning algorithms also have the reputation for being data- and computation-hungry for large-scale applications.  This project will address this issue by studying the important question of how to make reinforcement-learning algorithms scalable via introducing multiple learning agents and allowing them to collect data and learn optimal strategies collaboratively.  The outcomes of this project will have impacts on numerous areas where reinforcement learning is used at a scale, e.g., multi-phase clinical trials, training autonomous-driving algorithms, crowdsourcing tasks, pricing, and assortment optimization for stores at different locations.  The research products will be disseminated via talks at academic conferences and workshops, universities, industrial labs, and online media, and will also be integrated in two courses on the forefront of reinforcement learning and big-data algorithms.&lt;br/&gt;&lt;br/&gt;More technically, this project will study how to address the fundamental constraints on communication and adaptivity for the learning agents.  In particular, this project will investigate a handful of collaborative learning models, including full communication, synchronized communication, synchronized communication with limited adaptivity, and asynchronized communication, and study the following general questions: (1) what is the fundamental advantage of allowing adaptivity in the parallel learning model; (2) are there inherent differences on the degree of parallelism between model-based and model-free reinforcement learning; (3) what is the impact of asynchronized communication; and (4) is it possible to communication-efficiently parallelize general algorithmic techniques in reinforcement learning?  The team of researchers will address these questions by studying a set of core problems, including best arm(s) identification and regret minimization in multi-armed bandits, contextual bandits, finite-state Markov decision process (MDP) learning, reinforcement learning with function approximates, and coordinated exploration in MDPs.  Through studying these questions, this project will bring new techniques, perspectives, and insight to communication-efficient parallel reinforcement learning.  This project will also have a significant impact on a number of related research areas such as control theory, operations research, information theory and communication complexity, and multi-agent systems.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>08/05/2020</MinAmdLetterDate>
<MaxAmdLetterDate>08/05/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2006591</AwardID>
<Investigator>
<FirstName>Qin</FirstName>
<LastName>Zhang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Qin Zhang</PI_FULL_NAME>
<EmailAddress><![CDATA[qzhangcs@indiana.edu]]></EmailAddress>
<NSF_ID>000662627</NSF_ID>
<StartDate>08/05/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>BLOOMINGTON</CityName>
<ZipCode>474057000</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>107 S INDIANA AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>YH86RTW2YVJ4</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Indiana University]]></Name>
<CityName>Bloomington</CityName>
<StateCode>IN</StateCode>
<ZipCode>474083901</ZipCode>
<StreetAddress><![CDATA[700 North Woodlawn Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>2878</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~242214</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Bandit theory and reinforcement learning have shown significant success in a variety of practical applications, such as advertisement platforms, recommendation systems, dynamic pricing, etc. Large-scale reinforcement-learning algorithms, however, have a reputation for being data- and computation-hungry. This project has endeavored to tackle this key issue by investigating ways to scale reinforcement-learning algorithms by making learning in batches or introducing multiple learning agents and allowing them to collect data and develop learning strategies in a collaborative manner.&nbsp; The first model is called the batched learning model, and the second is named the collaborative learning model.&nbsp; In this project, we have made significant progress on both frontiers.</p> <p><br />In the collaborative learning model, we have conducted a systematic study on several fundamental questions, including: (1) What is the optimal tradeoff between the speedup of the collaboration and the amount of communication the agents need to exchange? (2) Are adaptive collaborative learning algorithms more powerful than non-adaptive ones? (3) Is collaborative learning in the heterogeneous environment more difficult than that in the homogeneous environment?&nbsp; We have answered these questions by the following results: (1) We have obtained almost tight tradeoffs between the communication between the agents and the speedup of the collaboration for two basic problems in bandit theory - the best arm identification and the top-m arm identifications in multi-armed bandits. (2) We have proved that adaptive collaborative learning algorithms are more powerful than non-adaptive collaborative learning algorithms in the homogeneous environment with respect to the number of communication rounds between the agents for achieving the same amount of speedup. (3) We have proved that collaborative learning in the heterogeneous environment is inherently more difficult than that in the homogeneous environment.</p> <p><br />In the batched learning model, we have studied several fundamental problems in the bandit theory, including best arm identification in multi-armed bandits, coarse ranking in multi-armed bandits, and multinomial logit bandit. We have given almost tight bounds for the tradeoffs between the batch complexity and the learning time for best arm identification and coarse ranking, and almost tight batch complexity under near-optimal regret for multinomial logit Bandit.</p> <p><br />The results in this project have been disseminated to the science disciplines and community at large through publications, open source libraries, and presentations. Several graduate students received support from this project and engaged in various research activities as research assistants, summer interns, and research-focused workshops. Two PhD theses were produced as outcomes of this research project. All publications and code libraries are available at the project website: https://homes.luddy.indiana.edu/qzhangcs/parallelRL.html</p> <p>&nbsp;</p><br> <p>  Last Modified: 11/09/2023<br> Modified by: Qin&nbsp;Zhang</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Bandit theory and reinforcement learning have shown significant success in a variety of practical applications, such as advertisement platforms, recommendation systems, dynamic pricing, etc. Large-scale reinforcement-learning algorithms, however, have a reputation for being data- and computation-hungry. This project has endeavored to tackle this key issue by investigating ways to scale reinforcement-learning algorithms by making learning in batches or introducing multiple learning agents and allowing them to collect data and develop learning strategies in a collaborative manner. The first model is called the batched learning model, and the second is named the collaborative learning model. In this project, we have made significant progress on both frontiers.    In the collaborative learning model, we have conducted a systematic study on several fundamental questions, including: (1) What is the optimal tradeoff between the speedup of the collaboration and the amount of communication the agents need to exchange? (2) Are adaptive collaborative learning algorithms more powerful than non-adaptive ones? (3) Is collaborative learning in the heterogeneous environment more difficult than that in the homogeneous environment? We have answered these questions by the following results: (1) We have obtained almost tight tradeoffs between the communication between the agents and the speedup of the collaboration for two basic problems in bandit theory - the best arm identification and the top-m arm identifications in multi-armed bandits. (2) We have proved that adaptive collaborative learning algorithms are more powerful than non-adaptive collaborative learning algorithms in the homogeneous environment with respect to the number of communication rounds between the agents for achieving the same amount of speedup. (3) We have proved that collaborative learning in the heterogeneous environment is inherently more difficult than that in the homogeneous environment.    In the batched learning model, we have studied several fundamental problems in the bandit theory, including best arm identification in multi-armed bandits, coarse ranking in multi-armed bandits, and multinomial logit bandit. We have given almost tight bounds for the tradeoffs between the batch complexity and the learning time for best arm identification and coarse ranking, and almost tight batch complexity under near-optimal regret for multinomial logit Bandit.    The results in this project have been disseminated to the science disciplines and community at large through publications, open source libraries, and presentations. Several graduate students received support from this project and engaged in various research activities as research assistants, summer interns, and research-focused workshops. Two PhD theses were produced as outcomes of this research project. All publications and code libraries are available at the project website: https://homes.luddy.indiana.edu/qzhangcs/parallelRL.html        Last Modified: 11/09/2023       Submitted by: QinZhang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
