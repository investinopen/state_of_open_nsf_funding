<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Inference in High-Dimensional Statistical Models: Algorithmic Tractability and Computational Barriers]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>200000.00</AwardTotalIntnAmount>
<AwardAmount>200000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yong Zeng</SignBlockName>
<PO_EMAI>yzeng@nsf.gov</PO_EMAI>
<PO_PHON>7032927299</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[Extracting knowledge from data using statistical and machine learning methods often involves computations, which don't scale well with dataset sizes. This is dictated by the necessity of analyzing large scale statistical models, where the scale of the data ever increases due to our unprecedented ability to accumulative massive amounts of it. Often this leads to models where the number of parameters far exceeds the amount of collected data,  rendering many classical inference models ill-posed and classical computational methods prohibitively time consuming. Thus the value brought about by the abundance of data comes at the expense of the necessity to develop completely novel computational tools that are capable of dealing with the curse of dimensionality. While there is  an abundance  of literature devoted to designing efficient computational methods of inference in high-dimensional statistical models, it was discovered that many algorithms hit a certain computational barrier, beyond which seemingly only brute-force and thus computationally prohibitive algorithms can succeed. Not much is known regarding the fundamental computational limitations arising above this barrier, which is  popularly dubbed  the nformation Theoretic vs Computation gap. What is the origin of this barrier? Does it indeed correspond to the onset of algorithmically intractable problems, or is it just a matter of being more clever about designing faster algorithms? The project also provides research training opportunities for graduate students. &lt;br/&gt;&lt;br/&gt;In the present project the PI develops a completely novel approach for understanding fundamental computational barriers arising in high dimensional statistical models. The approach  is based on powerful and illuminating insights derived from the field of statistical physics,  specifically the theory of spin glasses. In particular, the PI intends to establish that the onset of the algorithmic barriers is caused by phase transition in the landscape of the solution space, marking a drastic change in the solution space geometry of  underlying inference problems. This change in geometry of the solution space landscape taking the form of the so-called Overlap Gap Property (OGP), can further be used to rule out broad classes of algorithms as potential contenders to bridge the information theoretic and algorithmic gap. These classes of algorithms include  algorithms based on local improvements, such as  Gradient Descent and Stochastic Gradient Descend algorithms, algorithms based on Markov Chain Monte Carlo Method, algorithms broadly defined as Approximate Message Passing iterations, and algorithms based on constructing low-degree polynomials. The PI in particular intends to investigate the validity of a bold conjecture stating that for most, if not all of the known  models exhibiting apparent algorithmic barriers, the onset of this barrier coincides with the onset of the OGP. The PI intends to investigate this conjecture in the context of several widely studied modern models of high dimensional statistics and machine learning fields, including the Stochastic Block Model, the Spiked Tensor Model, and Wide Neural Networks model. All of these models are known to exhibit an apparent algorithmic hardness in some parameter regimes and thus these models offer a valuable framework for investigating the validity of the aforementioned conjecture, as well as algorithmic intractability implications.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>06/17/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/17/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2015517</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Gamarnik</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David Gamarnik</PI_FULL_NAME>
<EmailAddress><![CDATA[gamarnik@mit.edu]]></EmailAddress>
<NSF_ID>000136664</NSF_ID>
<StartDate>06/17/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>CAMBRIDGE</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress><![CDATA[77 MASSACHUSETTS AVE]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>E2NYLCDML6V1</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>E2NYLCDML6V1</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress><![CDATA[77 Massachusetts Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~200000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The work underlying this project led to development of&nbsp; a completely novel approach for understanding fundamental computational barriers arising in high dimensional statistical and probabilistic models. The approach is based on powerful and illuminating insights derived from the field of statistical physics, specifically the theory of spin glasses. In a series of papers underlying this project, the PI and his collaborators established that the onset of the algorithmic barriers is caused by phase transition in the landscape of the solution space, marking a drastic change in the solution space geometry of underlying inference problems. This change in geometry of the solution space landscape taking the form of the so-called Overlap Gap Property (OGP), was used further to rule out broad classes of algorithms as potential contenders to bridge the information theoretic and algorithmic gap in many settings such as optimization on random graphs and perceptron models.&nbsp;</p> <p>&nbsp;</p> <p>The project was very interdisciplinary in nature, and in addition to the core areas of statistics and algorithms the project results have ramification to the fields of probability theory, statistical physics and the field of quantum computing and quantum information science. In the probability theory it led to a plethora of new structural results concerning high dimensional probabilistic models such as random graphs and high dimensional perceptrons. Throught the work of this project the probabilistic techniques such as the second moment method,&nbsp; concentration inequality, Lindeberg's interpolation method gained a new strength and applicability. In statistical physics we have discovered the new types of phase transition uknown in the field of physics prior to our work, and furthermore, we have managed to obtain a complete characterization of types of spin glass models for which the optimization is an algorithmically tractable problem. In the field of quantum computing we have established fundamental barriers that quantum algorithms need to overcome in order to become successful in solving optimization problems of interest.</p> <p>&nbsp;</p> <p>We believe that the developments underyling project mark a completely new era in our understanding of algorithmic tractability of the optimization problems in general and in the context of high-dimensional probabilistic and statistical models in particular.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>  Last Modified: 12/07/2023<br> Modified by: David&nbsp;Gamarnik</p></div> <div class="porSideCol" ></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The work underlying this project led to development of a completely novel approach for understanding fundamental computational barriers arising in high dimensional statistical and probabilistic models. The approach is based on powerful and illuminating insights derived from the field of statistical physics, specifically the theory of spin glasses. In a series of papers underlying this project, the PI and his collaborators established that the onset of the algorithmic barriers is caused by phase transition in the landscape of the solution space, marking a drastic change in the solution space geometry of underlying inference problems. This change in geometry of the solution space landscape taking the form of the so-called Overlap Gap Property (OGP), was used further to rule out broad classes of algorithms as potential contenders to bridge the information theoretic and algorithmic gap in many settings such as optimization on random graphs and perceptron models.      The project was very interdisciplinary in nature, and in addition to the core areas of statistics and algorithms the project results have ramification to the fields of probability theory, statistical physics and the field of quantum computing and quantum information science. In the probability theory it led to a plethora of new structural results concerning high dimensional probabilistic models such as random graphs and high dimensional perceptrons. Throught the work of this project the probabilistic techniques such as the second moment method, concentration inequality, Lindeberg's interpolation method gained a new strength and applicability. In statistical physics we have discovered the new types of phase transition uknown in the field of physics prior to our work, and furthermore, we have managed to obtain a complete characterization of types of spin glass models for which the optimization is an algorithmically tractable problem. In the field of quantum computing we have established fundamental barriers that quantum algorithms need to overcome in order to become successful in solving optimization problems of interest.      We believe that the developments underyling project mark a completely new era in our understanding of algorithmic tractability of the optimization problems in general and in the context of high-dimensional probabilistic and statistical models in particular.              Last Modified: 12/07/2023       Submitted by: DavidGamarnik]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
