<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[NRI: INT: Customizing Semi-Autonomous Nursing Robots Using Human Expertise]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>701142.00</AwardTotalIntnAmount>
<AwardAmount>701142</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wendy Nilsen</SignBlockName>
<PO_EMAI>wnilsen@nsf.gov</PO_EMAI>
<PO_PHON>7032922568</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[Remote-controlled robots have the potential to allow humans to perform useful tasks without putting themselves in danger, or without travelling long distances. This project explores how humans can control nursing robots that can communicate with patients, collect vital signs, and perform routine cleaning tasks in quarantine environments. The use of these robots has the potential to protect nurses from infection during disease outbreaks, and to protect patients with weakened immune system. A significant challenge in this effort is to make the user interface to the robot easy enough for nurses to use without significant training. Because engineers are not experts in nursing, the research will let nurses customize the user interface by teaching the robot about objects, places, and tasks that are typically used in nursing. After training, artificial intelligence algorithms will then automatically estimate which actions the nurse wants to perform, and these will be presented in a simple user interface that allows the nurse to select those actions quickly. &lt;br/&gt;&lt;br/&gt;This project will continue an interdisciplinary collaboration between Duke's School of Engineering and School of Nursing. Research will be conducted in three thrust areas: 1) smart human operator interfaces for supervised autonomy that learn mappings between multimodal sensor input streams to provide simple, interpretable task options and status feedback; 2) hierarchical task learning algorithms for helping human experts train novel composite tasks; and 3) real world evaluation of human-robot system speed, reliability, operator workload, and operator learning curve using registered nurses and nursing students performing simulated clinical tasks in training environments.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>06/30/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/30/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2025782</AwardID>
<Investigator>
<FirstName>Kris</FirstName>
<LastName>Hauser</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kris Hauser</PI_FULL_NAME>
<EmailAddress><![CDATA[kkhauser@illinois.edu]]></EmailAddress>
<NSF_ID>000554065</NSF_ID>
<StartDate>06/30/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>URBANA</CityName>
<ZipCode>618013620</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>506 S WRIGHT ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>Y8CWNJRCNN91</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champai]]></Name>
<CityName>Champaign</CityName>
<StateCode>IL</StateCode>
<ZipCode>618207406</ZipCode>
<StreetAddress><![CDATA[1901 South First Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>063Z</Code>
<Text>FW-HTF Futr Wrk Hum-Tech Frntr</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01001819DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2018~701141</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The purpose of this project was to investigate methods to enhance how humans remotely operate robots to perform navigation, manipulation, and communication activities in healthcare settings. Although mobile manipulator robots are relatively straightforward to construct, operating each of their large numbers of degrees of freedom and sensors is overwhelming particularly for novice operators. Moreover, the precision and dexterity of such robots is limited by the characteristics of the operator interface device, such as a mouse and keyboard, haptic device, or VR controller, and network latency which degrade the operator experience. This research explored shared control strategies, which offload some responsibility of the control onto the robot, and in particular it studied how to customize the sharing of control to meet the operator's preferences.</p> <p>These studies used the Tele-Robotic Intelligent Nursing System (TRINA) platform to reach a variety of interesting findings. The success of later versions of the robot contribute evidence to the hypothesis that matching the robot to a human's size, reach, strength, and sensing allows for novice users to understand the capabilities of the robot more quickly and operate it more proficiently. Moreover, experiments show that shared control of low-level motor function, such as gripper orientation control, helps operators perform fine manipulation tasks, and that AI-based methods are capable of identifying target objects in the robot's environment and are capable of predicting users' intended actions with a high degree of accuracy (90%+). This latter result is promising because an AI-based predictive menu can help overcome the "operator overload" that would be experienced if dozens or hundreds of semi-autonomous actions available in a normal menu system.</p> <p>Broader impacts include a highly successful undergraduate research program that involved over 30 undergraduate students over four years on a team that prepared the TRINA robot for the ANA Avatar XPRIZE competition. In this interational competition, a novice judge was trained in 45 minutes to use a robot in a remote environment. Our team placed fourth in the competition and was one of only four teams to complete all 10 tasks. Ultimately, the competition exposed these undergraduate students with hands-on experience in topics in robotics, AR/VR, hardware design, and control, with many students proceeding into graduate-level studies in computing or engineering discplines.</p><br> <p>  Last Modified: 01/12/2024<br> Modified by: Kris&nbsp;Hauser</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)          </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2024/2025782/2025782_10572972_1705087263238_predictive_menu--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2025782/2025782_10572972_1705087263238_predictive_menu--rgov-800width.jpg" title="Immersive AI-based predictive menu"><img src="/por/images/Reports/POR/2024/2025782/2025782_10572972_1705087263238_predictive_menu--rgov-66x44.jpg" alt="Immersive AI-based predictive menu"></a> <div class="imageCaptionContainer"> <div class="imageCaption">While wearing VR headset, users of our system choose tasks for the robot to execute using a novel predictive menu system. The predictions depend on the observed objects in the world and are trained using machine learning from expert data.</div> <div class="imageCredit">UIUC Intelligent Motion Lab</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Kris&nbsp;Hauser <div class="imageTitle">Immersive AI-based predictive menu</div> </div> </li><li> <a href="/por/images/Reports/POR/2024/2025782/2025782_10572972_1705086829147_trina_three_generations_4x3--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2025782/2025782_10572972_1705086829147_trina_three_generations_4x3--rgov-800width.jpg" title="Three generations of TRINA platform"><img src="/por/images/Reports/POR/2024/2025782/2025782_10572972_1705086829147_trina_three_generations_4x3--rgov-66x44.jpg" alt="Three generations of TRINA platform"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Based on earlier work on the TRINA 1.0 platform, this project completed two upgraded versions of the robot to address mobility, dexterity, and usability issues.</div> <div class="imageCredit">UIUC Intelligent Motion Lab</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Kris&nbsp;Hauser <div class="imageTitle">Three generations of TRINA platform</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The purpose of this project was to investigate methods to enhance how humans remotely operate robots to perform navigation, manipulation, and communication activities in healthcare settings. Although mobile manipulator robots are relatively straightforward to construct, operating each of their large numbers of degrees of freedom and sensors is overwhelming particularly for novice operators. Moreover, the precision and dexterity of such robots is limited by the characteristics of the operator interface device, such as a mouse and keyboard, haptic device, or VR controller, and network latency which degrade the operator experience. This research explored shared control strategies, which offload some responsibility of the control onto the robot, and in particular it studied how to customize the sharing of control to meet the operator's preferences.   These studies used the Tele-Robotic Intelligent Nursing System (TRINA) platform to reach a variety of interesting findings. The success of later versions of the robot contribute evidence to the hypothesis that matching the robot to a human's size, reach, strength, and sensing allows for novice users to understand the capabilities of the robot more quickly and operate it more proficiently. Moreover, experiments show that shared control of low-level motor function, such as gripper orientation control, helps operators perform fine manipulation tasks, and that AI-based methods are capable of identifying target objects in the robot's environment and are capable of predicting users' intended actions with a high degree of accuracy (90%+). This latter result is promising because an AI-based predictive menu can help overcome the "operator overload" that would be experienced if dozens or hundreds of semi-autonomous actions available in a normal menu system.   Broader impacts include a highly successful undergraduate research program that involved over 30 undergraduate students over four years on a team that prepared the TRINA robot for the ANA Avatar XPRIZE competition. In this interational competition, a novice judge was trained in 45 minutes to use a robot in a remote environment. Our team placed fourth in the competition and was one of only four teams to complete all 10 tasks. Ultimately, the competition exposed these undergraduate students with hands-on experience in topics in robotics, AR/VR, hardware design, and control, with many students proceeding into graduate-level studies in computing or engineering discplines.     Last Modified: 01/12/2024       Submitted by: KrisHauser]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
