<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[EAGER: AF:Small: Algorithms for Relational Machine Learning]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>148828.00</AwardTotalIntnAmount>
<AwardAmount>148828</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Brass</SignBlockName>
<PO_EMAI>pbrass@nsf.gov</PO_EMAI>
<PO_PHON>7032922182</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[Relational database-management systems constitute a mature, ubiquitous, sophisticated technology that is deeply entrenched.  Seemingly all organizations are collecting vastly increasing volumes of structured as well as unstructured data, and want to extract knowledge from this data using machine-learning techniques/algorithms.  Thus many learning tasks faced by working data scientists involve relational data.  Thus a marriage of machine learning and relational databases seems inevitable.  However, standard machine-learning algorithms are not designed to  operate directly on relational data, and further, it is far from obvious if and how one can adapt many of these algorithms to work on relational data without suffering a significant loss of efficiency.  The current standard practice for a data scientist, confronted with a machine-learning task on relational data, is to  issue a feature-extraction query to extract the (carefully curated) data from the relational database by joining together multiple tables to create a design matrix, and then to import this design matrix  into some  machine-learning tool  to train the model.  This standard practice is wasteful because (1) computing  relational joins is computationally expensive, both in terms of time and space, (2) the resulting design matrix will likely contain much redundant information and consume much more space than the original  tables, and thus (3) the machine-learning task takes more time than should conceptually be necessary.  Algorithms that are orders of magnitude faster for standard machine-learning problems on relational data are certain to exist, and the goal of this research program is to discover them.  Such algorithms would allow the extraction of information from data that is now not currently feasibly extractable.&lt;br/&gt;&lt;br/&gt;The research goals of this project are threefold.  The first goal is to design and analyze relational algorithms for common machine-learning queries. A relational algorithm works directly on the relational data, without forming the design matrix, and can be orders of magnitude faster than standard machine-learning practice for such data.  The second goal  is to design a layer of relational algorithms for commonly arising subproblems and that can be utilized by the data scientists as a sort of middleware toolkit when designing their algorithms.  The third goal is to  build some intuition as to what problems are, and are not, solvable by relational algorithms and that researchers/practitioners can rely on when faced with a new problem.  These goals will require the development of new algorithmic-design and -analysis techniques.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>08/20/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/14/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2036077</AwardID>
<Investigator>
<FirstName>Kirk</FirstName>
<LastName>Pruhs</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kirk Pruhs</PI_FULL_NAME>
<EmailAddress><![CDATA[kirk@cs.pitt.edu]]></EmailAddress>
<NSF_ID>000388694</NSF_ID>
<StartDate>08/20/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pittsburgh</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152600001</ZipCode>
<PhoneNumber>4126247400</PhoneNumber>
<StreetAddress>4200 FIFTH AVENUE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA12</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>MKAGLD59JRL1</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF PITTSBURGH - OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pittsburgh]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>152133502</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>2878</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~148828</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Most&nbsp; of the major database players&nbsp; have database systems with learning incorporated into them.&nbsp; These systems rely on standard practice for evaluating a data science query, in that they first evaluate the database query and then subsequently evaluate the learning query.&nbsp; Independent of the learning task, this standard practice necessarily has exponential worst-case time and space complexity as the design matrix can be exponentially larger than the underlying relational tables.&nbsp; However, some data science queries can be efficiently evaluated&nbsp; (say in polynomial time if the structure of the tables is not too complicated) by what we call a relational algorithm, that operates directly on the relational tables, but without joining them.&nbsp; The goal of this research was to design and analyze such relational algorithms, with a particular focus on common learning tasks. <br /><br />One type of basic geometric problem that commonly arises in learning settings is computing something that can be modeled as a sum-product query on the collection of points satisfying some of additive constraints We developed a relational algorithm for approximately evaluating the most commonly arising sum-product queries with one additive restraint.&nbsp; Building on this foundational result, we were able to design a relational algorithm for the classic k-means clustering problem. <br /><br />Gradient descent is one of the workhorse algorithms for machine learning. We were able to show that under standard complexity theoretic assumptions, there can be no relational algorithm to compute the gradient for common machine learning objectives, such as the Support-Vector-Machine objective (SVM).&nbsp; But we were able to develop a beyond worst-case approach to obtain a reasonable positive result.&nbsp; In particular we developed a relational algorithm that computes a ``pseudo-gradient'' that on stable instances guarantees a&nbsp; convergence at a rate comparable to the standard bound on the convergence rate when using the real gradient, and<br />a solution that is nearly optimal. <br /><br />As part of this process we designed relation algorithms for most of the most important learning tasks. We also developed to algorithm design and analysis techniques that presumably will be employed by future researchers.&nbsp; A summary can be found in the adjacent figure: green represents prior knowledge, red represents known&nbsp; algorithm techniques/tools that we have discovered are useful for designing relational algorithms, or new algorithmic techniques that we have at least arguably invented, and blue represents new relational algorithms for classic learning problems. <br /><br />One PhD student was trained in the area of relational algorithms. <br /><br /><br /><br /></p><br> <p>  Last Modified: 01/25/2024<br> Modified by: Kirk&nbsp;Pruhs</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2024/2036077/2036077_10700439_1706213140631_techniques2--rgov-214x142.jpg" original="/por/images/Reports/POR/2024/2036077/2036077_10700439_1706213140631_techniques2--rgov-800width.jpg" title="Summary of Results"><img src="/por/images/Reports/POR/2024/2036077/2036077_10700439_1706213140631_techniques2--rgov-66x44.jpg" alt="Summary of Results"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Summary of Results</div> <div class="imageCredit">Me</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Kirk&nbsp;Pruhs <div class="imageTitle">Summary of Results</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Most of the major database players have database systems with learning incorporated into them. These systems rely on standard practice for evaluating a data science query, in that they first evaluate the database query and then subsequently evaluate the learning query. Independent of the learning task, this standard practice necessarily has exponential worst-case time and space complexity as the design matrix can be exponentially larger than the underlying relational tables. However, some data science queries can be efficiently evaluated (say in polynomial time if the structure of the tables is not too complicated) by what we call a relational algorithm, that operates directly on the relational tables, but without joining them. The goal of this research was to design and analyze such relational algorithms, with a particular focus on common learning tasks.   One type of basic geometric problem that commonly arises in learning settings is computing something that can be modeled as a sum-product query on the collection of points satisfying some of additive constraints We developed a relational algorithm for approximately evaluating the most commonly arising sum-product queries with one additive restraint. Building on this foundational result, we were able to design a relational algorithm for the classic k-means clustering problem.   Gradient descent is one of the workhorse algorithms for machine learning. We were able to show that under standard complexity theoretic assumptions, there can be no relational algorithm to compute the gradient for common machine learning objectives, such as the Support-Vector-Machine objective (SVM). But we were able to develop a beyond worst-case approach to obtain a reasonable positive result. In particular we developed a relational algorithm that computes a ``pseudo-gradient'' that on stable instances guarantees a convergence at a rate comparable to the standard bound on the convergence rate when using the real gradient, and a solution that is nearly optimal.   As part of this process we designed relation algorithms for most of the most important learning tasks. We also developed to algorithm design and analysis techniques that presumably will be employed by future researchers. A summary can be found in the adjacent figure: green represents prior knowledge, red represents known algorithm techniques/tools that we have discovered are useful for designing relational algorithms, or new algorithmic techniques that we have at least arguably invented, and blue represents new relational algorithms for classic learning problems.   One PhD student was trained in the area of relational algorithms.          Last Modified: 01/25/2024       Submitted by: KirkPruhs]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
