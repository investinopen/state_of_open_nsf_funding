<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[RI: Small: Learning discrete structure from continuous spaces]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/10/2020</AwardEffectiveDate>
<AwardExpirationDate>07/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>270763.00</AwardTotalIntnAmount>
<AwardAmount>270763</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Vladimir Pavlovic</SignBlockName>
<PO_EMAI>vpavlovi@nsf.gov</PO_EMAI>
<PO_PHON>7032928318</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[Science, medicine, business, and engineering are increasingly data-driven. Hypotheses, diagnoses, decisions, and designs are made by gathering and analyzing a wealth of data in search of meaningful patterns. It is the goal of the field of machine learning to develop methods for inferring and reasoning about patterns in the data. The research supported by this award addresses the question of learning from unlabeled data, one of the more vexing problems of data science.  The PI's analyze and develop algorithms for problems such as clustering, i.e., finding groups of similar objects, as well as understanding continuously changing attributes in data. By integrating machine learning, modeling and geometric data analysis, this work injects new ideas and methodologies to modern data analysis, helps build practical algorithms for unsupervised and unsupervised learning and analyze their properties and domains of applicability. Students working on this project have a unique opportunity to be exposed to a broad spectrum of topics including machine learning, statistics, geometry  and applied mathematics.&lt;br/&gt;&lt;br/&gt;On a more technical level, the unifying  perspective  for the proposed research is that many of these unsupervised learning problems can be viewed as recovering structure or invariants of the underlying continuous space through the lens of the discrete data. This work takes that point of view to consider  a number of important aspects of unsupervised learning including hierarchical clustering in the density model, data quantization, graphon clustering and estimation, as well as learning metric structure from data. The project also considers applications of these ideas to supervised learning, particularly in helping to scale algorithms to large data.   While the work on this project concentrates on theoretical analyses, these  are developed with a view toward practical algorithms, implementations and applications.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>09/14/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/14/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2050360</AwardID>
<Investigator>
<FirstName>Mikhail</FirstName>
<LastName>Belkin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mikhail Belkin</PI_FULL_NAME>
<EmailAddress><![CDATA[mbelkin@ucsd.edu]]></EmailAddress>
<NSF_ID>000107334</NSF_ID>
<StartDate>09/14/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>LA JOLLA</CityName>
<ZipCode>920930021</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress><![CDATA[9500 GILMAN DR]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>50</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA50</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>UYTTZT6G9DT1</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>920930621</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>50</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA50</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01001819DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2018~270763</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The research supported by this proposal has made significant progress toward understanding geometric structures, specifically in optimization of deep learning system. To give just one example, we have made significant steps toward understanding optimization landscapes of over-parameterized systems, a key aspect of moder deep learning. We have showed that while such landscapes are almost never convex, even locally, they satisfy the classical Polyak-Lojasiewicz condition which allows local methods, such as Gradient Descent and even Stochastic Gradient Descent (SGD) to converge to a global minimum. These landscapes and the comparison between more traditional uder-parameterized and "modern" over-parameterized regimes are illustrated in the figures.</p> <p>Other significant theoretical advances include understanding generalization properties of infinitely wide, showing that such networks can achieve optimality for certain activation functions. Another finding was establishing a connection between deep over-parameterized neural autoencoders associative memory.Specifically, we showed that standard autoencoding neural networks can implement both storage and retrieval operations and even memorize full sequences.</p> <p>The funding provided by this grant also supported dissemination of modern theoretical progress to a wider audience through multiple activities including publication of a significant review paper outlining recent developments in deep learning theory (Belkin, Mikhail, Fit without fear: remarkable mathematical  phenomena of deep learning through the prism of interpolation. 30. <em>Acta Numerica, 2021). <br /></em></p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>  Last Modified: 11/29/2023<br> Modified by: Mikhail&nbsp;Belkin</p></div> <div class="porSideCol" ><div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)          </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2023/2050360/2050360_10567935_1700769594283_over_param--rgov-214x142.jpg" original="/por/images/Reports/POR/2023/2050360/2050360_10567935_1700769594283_over_param--rgov-800width.jpg" title="Over-parameterized landscape"><img src="/por/images/Reports/POR/2023/2050360/2050360_10567935_1700769594283_over_param--rgov-66x44.jpg" alt="Over-parameterized landscape"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Landscape for over-parameterized systems.</div> <div class="imageCredit">Loss landscapes and optimization in over-parameterized non-linear systems and neural networks, C.Liu, L. Zhu, M. Belkin, ACHA 2022</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Mikhail&nbsp;Belkin <div class="imageTitle">Over-parameterized landscape</div> </div> </li><li> <a href="/por/images/Reports/POR/2023/2050360/2050360_10567935_1700769158082_under_param--rgov-214x142.jpg" original="/por/images/Reports/POR/2023/2050360/2050360_10567935_1700769158082_under_param--rgov-800width.jpg" title="Under-parameterized landscape"><img src="/por/images/Reports/POR/2023/2050360/2050360_10567935_1700769158082_under_param--rgov-66x44.jpg" alt="Under-parameterized landscape"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Non-convex loss landscape of under-parameterized systems</div> <div class="imageCredit">Loss landscapes and optimization in over-parameterized non-linear systems and neural networks, C.Liu, L. Zhu, M. Belkin, ACHA 2022</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Mikhail&nbsp;Belkin <div class="imageTitle">Under-parameterized landscape</div> </div> </li></ul> </div> </div></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The research supported by this proposal has made significant progress toward understanding geometric structures, specifically in optimization of deep learning system. To give just one example, we have made significant steps toward understanding optimization landscapes of over-parameterized systems, a key aspect of moder deep learning. We have showed that while such landscapes are almost never convex, even locally, they satisfy the classical Polyak-Lojasiewicz condition which allows local methods, such as Gradient Descent and even Stochastic Gradient Descent (SGD) to converge to a global minimum. These landscapes and the comparison between more traditional uder-parameterized and "modern" over-parameterized regimes are illustrated in the figures.   Other significant theoretical advances include understanding generalization properties of infinitely wide, showing that such networks can achieve optimality for certain activation functions. Another finding was establishing a connection between deep over-parameterized neural autoencoders associative memory.Specifically, we showed that standard autoencoding neural networks can implement both storage and retrieval operations and even memorize full sequences.   The funding provided by this grant also supported dissemination of modern theoretical progress to a wider audience through multiple activities including publication of a significant review paper outlining recent developments in deep learning theory (Belkin, Mikhail, Fit without fear: remarkable mathematical  phenomena of deep learning through the prism of interpolation. 30. Acta Numerica, 2021).                Last Modified: 11/29/2023       Submitted by: MikhailBelkin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
